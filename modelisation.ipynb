{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T02:18:59.885889135Z",
     "start_time": "2026-02-06T02:18:59.651744643Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score, \n",
    "    precision_score, recall_score, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "# Mod√®les\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Gestion du d√©s√©quilibre\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T02:19:06.960988542Z",
     "start_time": "2026-02-06T02:18:59.887371792Z"
    }
   },
   "source": [
    "print(\"Chargement des donn√©es nettoy√©es\\n\")\n",
    "df_train = pd.read_csv('data_train_cleaned.csv')\n",
    "df_test = pd.read_csv('data_test_cleaned.csv')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des donn√©es nettoy√©es\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T02:19:07.185101589Z",
     "start_time": "2026-02-06T02:19:07.005626151Z"
    }
   },
   "source": [
    "print(\"PR√âPARATION DES FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Colonnes √† exclure\n",
    "exclude_cols = ['ZIBZIN', 'IDAvisAutorisationCheque', 'FlagImpaye', \n",
    "                'DateTransaction', 'CodeDecision','VerifianceCPT2', 'VerifianceCPT3']\n",
    "\n",
    "# Si 'Mois' et 'Date' existent (cr√©√©s dans le notebook EDA), les exclure aussi\n",
    "if 'Mois' in df_train.columns:\n",
    "    exclude_cols.append('Mois')\n",
    "if 'Date' in df_train.columns:\n",
    "    exclude_cols.append('Date')\n",
    "\n",
    "# Features disponibles\n",
    "feature_cols = [col for col in df_train.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\nFeatures s√©lectionn√©es ({len(feature_cols)}):\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\" {i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\n Colonnes exclues: {exclude_cols}\")\n",
    "print(\" CodeDecision exclu car information post-transaction = FUITE!\")\n",
    "\n",
    "# Cr√©er X et y\n",
    "X_train = df_train[feature_cols].copy()\n",
    "y_train = df_train['FlagImpaye'].copy()\n",
    "\n",
    "X_test = df_test[feature_cols].copy()\n",
    "y_test = df_test['FlagImpaye'].copy()\n",
    "\n",
    "print(f\"\\n‚úì Features extraites:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR√âPARATION DES FEATURES\n",
      "======================================================================\n",
      "\n",
      "Features s√©lectionn√©es (16):\n",
      "  1. Montant\n",
      "  2. VerifianceCPT1\n",
      "  3. D2CB\n",
      "  4. ScoringFP1\n",
      "  5. ScoringFP2\n",
      "  6. ScoringFP3\n",
      "  7. TauxImpNb_RB\n",
      "  8. TauxImpNB_CPM\n",
      "  9. EcartNumCheq\n",
      " 10. NbrMagasin3J\n",
      " 11. DiffDateTr1\n",
      " 12. DiffDateTr2\n",
      " 13. DiffDateTr3\n",
      " 14. CA3TRetMtt\n",
      " 15. CA3TR\n",
      " 16. Heure\n",
      "\n",
      " Colonnes exclues: ['ZIBZIN', 'IDAvisAutorisationCheque', 'FlagImpaye', 'DateTransaction', 'CodeDecision', 'VerifianceCPT2', 'VerifianceCPT3', 'Mois', 'Date']\n",
      " CodeDecision exclu car information post-transaction = FUITE!\n",
      "\n",
      "‚úì Features extraites:\n",
      "  X_train: (3888468, 16)\n",
      "  X_test:  (737068, 16)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T02:19:13.184173312Z",
     "start_time": "2026-02-06T02:19:07.186821528Z"
    }
   },
   "source": [
    "# Imputation des valeurs manquantes\n",
    "print(\"\\nImputation des valeurs manquantes...\")\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_imputed = pd.DataFrame(\n",
    "    imputer.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "print(\"Imputation termin√©e\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputation des valeurs manquantes...\n",
      "Imputation termin√©e\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T02:19:13.660808262Z",
     "start_time": "2026-02-06T02:19:13.210429012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalisation\n",
    "print(\"\\nNormalisation (StandardScaler)...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "print(\"Normalisation termin√©e\")\n",
    "\n",
    "print(f\"\\nPr√©paration termin√©e!\")\n",
    "print(f\"X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"X_test_scaled:  {X_test_scaled.shape}\")\n",
    "print(f\"y_train: Fraudes = {(y_train==1).sum():,} / {len(y_train):,}\")\n",
    "print(f\"y_test:  Fraudes = {(y_test==1).sum():,} / {len(y_test):,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalisation (StandardScaler)...\n",
      "Normalisation termin√©e\n",
      "\n",
      "Pr√©paration termin√©e!\n",
      "X_train_scaled: (3888468, 16)\n",
      "X_test_scaled:  (737068, 16)\n",
      "y_train: Fraudes = 23,346 / 3,888,468\n",
      "y_test:  Fraudes = 6,485 / 737,068\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T02:19:13.685002141Z",
     "start_time": "2026-02-06T02:19:13.661638928Z"
    }
   },
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name, verbose=True):\n",
    "    \"\"\"\n",
    "    √âvalue un mod√®le et retourne les m√©triques\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        Mod√®le entra√Æn√©\n",
    "    X_test : array-like\n",
    "        Features de test\n",
    "    y_test : array-like\n",
    "        Labels de test\n",
    "    model_name : str\n",
    "        Nom du mod√®le\n",
    "    verbose : bool\n",
    "        Afficher les r√©sultats d√©taill√©s\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionnaire avec les m√©triques\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculer les m√©triques\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"R√âSULTATS - {model_name}\")\n",
    "        print(f\"F1-score:  {f1:.4f} ‚≠ê\")\n",
    "        print(f\"Pr√©cision: {precision:.4f}\")\n",
    "        print(f\"Rappel:    {recall:.4f}\")\n",
    "        print(f\"\\nMatrice de confusion:\")\n",
    "        print(f\"  TN: {tn:,}  |  FP: {fp:,}\")\n",
    "        print(f\"  FN: {fn:,}  |  TP: {tp:,}\")\n",
    "        print(f\"\\nInterpr√©tation:\")\n",
    "        print(f\"  ‚Ä¢ Vraies fraudes d√©tect√©es (TP): {tp:,}\")\n",
    "        print(f\"  ‚Ä¢ Fraudes manqu√©es (FN): {fn:,}\")\n",
    "        print(f\"  ‚Ä¢ Fausses alarmes (FP): {fp:,}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'tp': int(tp),\n",
    "        'fp': int(fp),\n",
    "        'fn': int(fn),\n",
    "        'tn': int(tn)\n",
    "    }\n",
    "\n",
    "print(\"Fonction d'√©valuation d√©finie\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonction d'√©valuation d√©finie\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Mod√©lisation\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T02:19:13.698078127Z",
     "start_time": "2026-02-06T02:19:13.687937281Z"
    }
   },
   "source": [
    "# Initialiser la liste des r√©sultats\n",
    "results = []"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Logistic Regression + class_weight"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T02:19:18.212180378Z",
     "start_time": "2026-02-06T02:19:13.699454448Z"
    }
   },
   "source": [
    "print(\"Logistic Regression\")\n",
    "#approche cost-sensitive: donne plus de poids √† la classe minoritaire\n",
    "model1 = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "model1.fit(X_train_scaled, y_train)\n",
    "print(\"Entra√Ænement termin√©\")\n",
    "\n",
    "result1 = evaluate_model(model1, X_test_scaled, y_test, \"LogReg + class_weight\")\n",
    "results.append(result1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Entra√Ænement termin√©\n",
      "R√âSULTATS - LogReg + class_weight\n",
      "F1-score:  0.0386 ‚≠ê\n",
      "Pr√©cision: 0.0200\n",
      "Rappel:    0.5806\n",
      "\n",
      "Matrice de confusion:\n",
      "  TN: 545,877  |  FP: 184,706\n",
      "  FN: 2,720  |  TP: 3,765\n",
      "\n",
      "Interpr√©tation:\n",
      "  ‚Ä¢ Vraies fraudes d√©tect√©es (TP): 3,765\n",
      "  ‚Ä¢ Fraudes manqu√©es (FN): 2,720\n",
      "  ‚Ä¢ Fausses alarmes (FP): 184,706\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Random Forest + class_weight"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T02:22:01.836070214Z",
     "start_time": "2026-02-06T02:19:18.236174132Z"
    }
   },
   "source": [
    "print(\"Random Forest\")\n",
    "#Ensemble d'arbres avec pond√©ration des classes\n",
    "\n",
    "model2 = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model2.fit(X_train_scaled, y_train)\n",
    "\n",
    "result2 = evaluate_model(model2, X_test_scaled, y_test, \"RandomForest + class_weight\")\n",
    "results.append(result2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "R√âSULTATS - RandomForest + class_weight\n",
      "F1-score:  0.0362 ‚≠ê\n",
      "Pr√©cision: 0.0186\n",
      "Rappel:    0.6534\n",
      "\n",
      "Matrice de confusion:\n",
      "  TN: 507,126  |  FP: 223,457\n",
      "  FN: 2,248  |  TP: 4,237\n",
      "\n",
      "Interpr√©tation:\n",
      "  ‚Ä¢ Vraies fraudes d√©tect√©es (TP): 4,237\n",
      "  ‚Ä¢ Fraudes manqu√©es (FN): 2,248\n",
      "  ‚Ä¢ Fausses alarmes (FP): 223,457\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### SMOTE + Logistic Regression"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T02:22:05.498633928Z",
     "start_time": "2026-02-06T02:22:01.865521844Z"
    }
   },
   "source": [
    "print(\"Logistic Regression\")\n",
    "#G√©n√©ration d'exemples synth√©tiques de la classe minoritaire\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.1, random_state=42)  # 10% du ratio\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Taille apr√®s SMOTE: {X_train_smote.shape[0]:,} exemples\")\n",
    "print(f\"Fraudes: {(y_train_smote==1).sum():,} ({(y_train_smote==1).sum()/len(y_train_smote)*100:.1f}%)\")\n",
    "model3 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model3.fit(X_train_smote, y_train_smote)\n",
    "print(\"Entra√Ænement termin√©\")\n",
    "\n",
    "result3 = evaluate_model(model3, X_test_scaled, y_test, \"SMOTE + LogReg\")\n",
    "results.append(result3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Taille apr√®s SMOTE: 4,251,634 exemples\n",
      "Fraudes: 386,512 (9.1%)\n",
      "Entra√Ænement termin√©\n",
      "R√âSULTATS - SMOTE + LogReg\n",
      "F1-score:  0.0941 ‚≠ê\n",
      "Pr√©cision: 0.1557\n",
      "Rappel:    0.0674\n",
      "\n",
      "Matrice de confusion:\n",
      "  TN: 728,213  |  FP: 2,370\n",
      "  FN: 6,048  |  TP: 437\n",
      "\n",
      "Interpr√©tation:\n",
      "  ‚Ä¢ Vraies fraudes d√©tect√©es (TP): 437\n",
      "  ‚Ä¢ Fraudes manqu√©es (FN): 6,048\n",
      "  ‚Ä¢ Fausses alarmes (FP): 2,370\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### RandomUnderSampler + Logistic Regression"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T02:22:06.192696311Z",
     "start_time": "2026-02-06T02:22:05.525539005Z"
    }
   },
   "source": [
    "print(\"M√âTHODE 5: RandomUnderSampler (under-sampling) + Logistic Regression\")\n",
    "#R√©duction de la classe majoritaire\n",
    "\n",
    "# Under-sampling\n",
    "rus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Taille apr√®s under-sampling: {X_train_rus.shape[0]:,} exemples\")\n",
    "print(f\"Fraudes: {(y_train_rus==1).sum():,} ({(y_train_rus==1).sum()/len(y_train_rus)*100:.1f}%)\")\n",
    "model4 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model4.fit(X_train_rus, y_train_rus)\n",
    "result4 = evaluate_model(model4, X_test_scaled, y_test, \"UnderSampling + LogReg\")\n",
    "results.append(result4)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√âTHODE 5: RandomUnderSampler (under-sampling) + Logistic Regression\n",
      "Taille apr√®s under-sampling: 70,038 exemples\n",
      "Fraudes: 23,346 (33.3%)\n",
      "R√âSULTATS - UnderSampling + LogReg\n",
      "F1-score:  0.0670 ‚≠ê\n",
      "Pr√©cision: 0.0370\n",
      "Rappel:    0.3553\n",
      "\n",
      "Matrice de confusion:\n",
      "  TN: 670,566  |  FP: 60,017\n",
      "  FN: 4,181  |  TP: 2,304\n",
      "\n",
      "Interpr√©tation:\n",
      "  ‚Ä¢ Vraies fraudes d√©tect√©es (TP): 2,304\n",
      "  ‚Ä¢ Fraudes manqu√©es (FN): 4,181\n",
      "  ‚Ä¢ Fausses alarmes (FP): 60,017\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Balanced Random Forest"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T02:22:26.950329653Z",
     "start_time": "2026-02-06T02:22:06.247505926Z"
    }
   },
   "source": [
    "print(\"Balanced Random Forest (imblearn)\")\n",
    "#Random Forest sp√©cialement con√ßu pour donn√©es d√©s√©quilibr√©es\n",
    "\n",
    "model5 = BalancedRandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model5.fit(X_train_scaled, y_train)\n",
    "result5 = evaluate_model(model5, X_test_scaled, y_test, \"BalancedRandomForest\")\n",
    "results.append(result5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Random Forest (imblearn)\n",
      "R√âSULTATS - BalancedRandomForest\n",
      "F1-score:  0.0350 ‚≠ê\n",
      "Pr√©cision: 0.0180\n",
      "Rappel:    0.6848\n",
      "\n",
      "Matrice de confusion:\n",
      "  TN: 487,793  |  FP: 242,790\n",
      "  FN: 2,044  |  TP: 4,441\n",
      "\n",
      "Interpr√©tation:\n",
      "  ‚Ä¢ Vraies fraudes d√©tect√©es (TP): 4,441\n",
      "  ‚Ä¢ Fraudes manqu√©es (FN): 2,044\n",
      "  ‚Ä¢ Fausses alarmes (FP): 242,790\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## XGBoost + SMOTE"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T02:22:26.977189800Z",
     "start_time": "2026-02-06T02:22:26.972225830Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T02:22:31.198523374Z",
     "start_time": "2026-02-06T02:22:26.977778422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#  XGBoost + SMOTE (BONUS - Plus performant)\n",
    "\n",
    "\n",
    "print(\"XGBoost + SMOTE\")\n",
    "#\"Gradient Boosting optimis√© (XGBoost) avec sur-√©chantillonnage\n",
    "\n",
    "# Installer XGBoost si pas encore fait (d√©commenter si n√©cessaire)\n",
    "# !pip install xgboost --break-system-packages\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Taille apr√®s SMOTE: {X_train_smote.shape[0]:,} exemples\")\n",
    "print(f\"Fraudes: {(y_train_smote==1).sum():,} ({(y_train_smote==1).sum()/len(y_train_smote)*100:.1f}%)\")\n",
    "\n",
    "# Configuration XGBoost (optimis√©e pour gros datasets)\n",
    "model6 = XGBClassifier(\n",
    "    n_estimators=50,        # Nombre d'arbres (r√©duit pour vitesse)\n",
    "    max_depth=5,            # Profondeur max\n",
    "    learning_rate=0.1,      # Taux d'apprentissage\n",
    "    subsample=0.8,          # Sous-√©chantillonnage pour r√©gularisation\n",
    "    colsample_bytree=0.8,   # Sous-√©chantillonnage des features\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',  # M√©trique d'√©valuation\n",
    "    tree_method='hist',     # M√©thode rapide pour gros datasets\n",
    "    n_jobs=-1               # Utiliser tous les CPU\n",
    ")\n",
    "\n",
    "model6.fit(X_train_smote, y_train_smote)\n",
    "# √âvaluation avec la fonction existante\n",
    "result6 = evaluate_model(model6, X_test_scaled, y_test, \"XGBoost + SMOTE\")\n",
    "results.append(result6)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost + SMOTE\n",
      "Taille apr√®s SMOTE: 4,251,634 exemples\n",
      "Fraudes: 386,512 (9.1%)\n",
      "R√âSULTATS - XGBoost + SMOTE\n",
      "F1-score:  0.1304 ‚≠ê\n",
      "Pr√©cision: 0.1568\n",
      "Rappel:    0.1116\n",
      "\n",
      "Matrice de confusion:\n",
      "  TN: 726,690  |  FP: 3,893\n",
      "  FN: 5,761  |  TP: 724\n",
      "\n",
      "Interpr√©tation:\n",
      "  ‚Ä¢ Vraies fraudes d√©tect√©es (TP): 724\n",
      "  ‚Ä¢ Fraudes manqu√©es (FN): 5,761\n",
      "  ‚Ä¢ Fausses alarmes (FP): 3,893\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### SMOTEENN + Logistic Regression"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-02-06T02:22:31.221885970Z"
    }
   },
   "source": [
    "print(\"SMOTEENN + Logistic Regression\")\n",
    "#Combinaison de SMOTE (over-sampling) et ENN (under-sampling)\n",
    "\n",
    "# SMOTEENN\n",
    "smoteenn = SMOTEENN(sampling_strategy=0.1, random_state=42)\n",
    "X_train_smoteenn, y_train_smoteenn = smoteenn.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Taille apr√®s SMOTEENN: {X_train_smoteenn.shape[0]:,} exemples\")\n",
    "print(f\"Fraudes: {(y_train_smoteenn==1).sum():,} ({(y_train_smoteenn==1).sum()/len(y_train_smoteenn)*100:.1f}%)\")\n",
    "\n",
    "model7 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model7.fit(X_train_smoteenn, y_train_smoteenn)\n",
    "print(\"Entra√Ænement termin√©\")\n",
    "\n",
    "result7 = evaluate_model(model7, X_test_scaled, y_test, \"SMOTEENN + LogReg\")\n",
    "results.append(result7)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTEENN + Logistic Regression\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gradient Boosting + sample_weight"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Gradient Boosting\")\n",
    "#Boosting avec √©chantillonnage strati√©\n",
    "\n",
    "# Calculer le ratio de d√©s√©quilibre\n",
    "nb_normales = (y_train == 0).sum()\n",
    "nb_fraudes = (y_train == 1).sum()\n",
    "ratio = nb_normales / nb_fraudes if nb_fraudes > 0 else 1\n",
    "\n",
    "# Cr√©er les poids\n",
    "class_weights = {0: 1, 1: ratio}\n",
    "sample_weights = np.array([class_weights[i] for i in y_train])\n",
    "\n",
    "print(f\"Ratio de d√©s√©quilibre: 1:{ratio:.1f}\")\n",
    "model8 = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "model8.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
    "print(\"Entra√Ænement termin√©\")\n",
    "\n",
    "result8 = evaluate_model(model8, X_test_scaled, y_test, \"GradientBoosting + sample_weight\")\n",
    "results.append(result8)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Comparaison des R√©sultats"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\" COMPARAISON FINALE DES 7 M√âTHODES\")\n",
    "\n",
    "# Cr√©er un DataFrame de r√©sultats\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('f1', ascending=False)\n",
    "\n",
    "print(\"\\n TABLEAU R√âCAPITULATIF (tri√© par F-mesure):\\n\")\n",
    "print(results_df[['model', 'f1', 'precision', 'recall', 'tp', 'fp', 'fn']].to_string(index=False))\n",
    "\n",
    "# Meilleur mod√®le\n",
    "best_model = results_df.iloc[0]\n",
    "print(f\"\\nüèÜ MEILLEUR MOD√àLE: {best_model['model']}\")\n",
    "print(f\" F-mesure: {best_model['f1']:.4f}\")\n",
    "print(f\" Pr√©cision: {best_model['precision']:.4f}\")\n",
    "print(f\" Rappel: {best_model['recall']:.4f}\")\n",
    "print(f\" TP (fraudes d√©tect√©es): {best_model['tp']:,}\")\n",
    "print(f\" FN (fraudes manqu√©es): {best_model['fn']:,}\")\n",
    "print(f\" FP (fausses alarmes): {best_model['fp']:,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualisation des r√©sultats\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# F1-score\n",
    "axes[0].barh(results_df['model'], results_df['f1'], color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Comparaison F1-Score', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "for i, v in enumerate(results_df['f1']):\n",
    "    axes[0].text(v, i, f' {v:.4f}', va='center', fontweight='bold')\n",
    "\n",
    "# Pr√©cision vs Rappel\n",
    "axes[1].scatter(results_df['recall'], results_df['precision'], s=200, alpha=0.6, c=results_df['f1'], \n",
    "                cmap='viridis', edgecolors='black', linewidth=2)\n",
    "for i, model in enumerate(results_df['model']):\n",
    "    axes[1].annotate(model, (results_df.iloc[i]['recall'], results_df.iloc[i]['precision']),\n",
    "                    fontsize=8, ha='right')\n",
    "axes[1].set_xlabel('Rappel', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Pr√©cision', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Pr√©cision vs Rappel', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Nombre de TP (fraudes d√©tect√©es)\n",
    "axes[2].barh(results_df['model'], results_df['tp'], color='green', alpha=0.7, edgecolor='black')\n",
    "axes[2].set_xlabel('Nombre de TP (fraudes d√©tect√©es)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('Fraudes Correctement D√©tect√©es', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(axis='x', alpha=0.3)\n",
    "for i, v in enumerate(results_df['tp']):\n",
    "    axes[2].text(v, i, f' {v:,}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse du Meilleur Mod√®le\n",
    "\n",
    "Analyse approfondie du mod√®le ayant obtenu la meilleure F-mesure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"ANALYSE APPROFONDIE DU MEILLEUR MOD√àLE\")\n",
    "\n",
    "best_model_name = best_model[\"model\"]\n",
    "print(f\"\\nMod√®le s√©lectionn√©: {best_model_name}\")\n",
    "\n",
    "# Mapping exact : label -> mod√®le entra√Æn√©\n",
    "trained_models = {\n",
    "    \"LogReg + class_weight\": model1,\n",
    "    \"RandomForest + class_weight\": model2,\n",
    "    \"SMOTE + LogReg\": model3,\n",
    "    \"UnderSampling + LogReg\": model4,\n",
    "    \"BalancedRandomForest\": model5,\n",
    "    \"XGBoost + SMOTE\": model6,\n",
    "    \"SMOTEENN + LogReg\": model7,\n",
    "    \"GradientBoosting + sample_weight\": model8,\n",
    "}\n",
    "\n",
    "# R√©cup√©rer le mod√®le\n",
    "best_trained_model = trained_models.get(best_model_name)\n",
    "\n",
    "if best_trained_model is None:\n",
    "    raise ValueError(\n",
    "        f\"Mod√®le '{best_model_name}' introuvable dans trained_models. \"\n",
    "        f\"Mod√®les dispo: {list(trained_models.keys())}\"\n",
    "    )\n",
    "\n",
    "print(f\"Objet mod√®le r√©cup√©r√©: {type(best_trained_model).__name__}\")\n",
    "\n",
    "\n",
    "# Matrice de confusion d√©taill√©e\n",
    "y_pred_best = best_trained_model.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Matrice de confusion\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "            xticklabels=['Normal', 'Fraude'], yticklabels=['Normal', 'Fraude'])\n",
    "axes[0].set_ylabel('Vraie Classe', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Classe Pr√©dite', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Matrice de Confusion', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Proportions\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Normal', 'Fraude'], yticklabels=['Normal', 'Fraude'])\n",
    "axes[1].set_ylabel('Vraie Classe', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Classe Pr√©dite', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Matrice de Confusion (Normalis√©e)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nüìã Rapport de classification d√©taill√©:\\n\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Normal', 'Fraude']))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Optimisation du Seuil de D√©cision\n",
    "\n",
    "Pour certains mod√®les, on peut optimiser le seuil de classification (par d√©faut 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"OPTIMISATION DU SEUIL DE D√âCISION\")\n",
    "# V√©rifier si le mod√®le a predict_proba\n",
    "if hasattr(best_trained_model, 'predict_proba'):\n",
    "    print(\"\\nLe meilleur mod√®le supporte predict_proba ‚Üí optimisation possible\")\n",
    "    \n",
    "    # Obtenir les probabilit√©s\n",
    "    y_proba = best_trained_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Tester diff√©rents seuils\n",
    "    seuils = np.linspace(0.1, 0.9, 17)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for seuil in seuils:\n",
    "        y_pred_seuil = (y_proba >= seuil).astype(int)\n",
    "        f1 = f1_score(y_test, y_pred_seuil)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Meilleur seuil\n",
    "    best_threshold_idx = np.argmax(f1_scores)\n",
    "    best_threshold = seuils[best_threshold_idx]\n",
    "    best_f1_threshold = f1_scores[best_threshold_idx]\n",
    "    \n",
    "    print(f\"\\n Seuil optimal trouv√©: {best_threshold:.2f}\")\n",
    "    print(f\"  F1-score avec seuil optimal: {best_f1_threshold:.4f}\")\n",
    "    print(f\"  F1-score avec seuil 0.5:     {best_model['f1']:.4f}\")\n",
    "    print(f\"  Am√©lioration: {(best_f1_threshold - best_model['f1']):.4f}\")\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(seuils, f1_scores, marker='o', linewidth=2, markersize=8)\n",
    "    plt.axvline(best_threshold, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Seuil optimal: {best_threshold:.2f}')\n",
    "    plt.axvline(0.5, color='gray', linestyle='--', linewidth=2, alpha=0.5,\n",
    "                label='Seuil par d√©faut: 0.50')\n",
    "    plt.xlabel('Seuil de d√©cision', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "    plt.title('Impact du Seuil de D√©cision sur la F-mesure', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"\\n Le meilleur mod√®le ne supporte pas predict_proba\")\n",
    "    print(\" Optimisation du seuil non applicable\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Sauvegarde des R√©sultats"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"SAUVEGARDE DES R√âSULTATS\")\n",
    "\n",
    "# Sauvegarder le tableau de r√©sultats\n",
    "results_df.to_csv('resultats_modelisation_partie1.csv', index=False)\n",
    "print(\"\\n‚úì R√©sultats sauvegard√©s: resultats_modelisation_partie1.csv\")\n",
    "\n",
    "# Cr√©er un r√©sum√© textuel\n",
    "with open('resume_modelisation.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"R√âSUM√â - MOD√âLISATION PARTIE 1: MAXIMISATION F-MESURE\\n\")\n",
    "    f.write(f\"Dataset:\\n\")\n",
    "    f.write(f\"  Train: {len(df_train):,} transactions ({train_fraudes:,} fraudes)\\n\")\n",
    "    f.write(f\"  Test:  {len(df_test):,} transactions ({test_fraudes:,} fraudes)\\n\")\n",
    "    f.write(f\"  Features utilis√©es: {len(feature_cols)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"R√âSULTATS PAR MOD√àLE:\\n\")\n",
    "    f.write(results_df[['model', 'f1', 'precision', 'recall', 'tp', 'fn', 'fp']].to_string(index=False))\n",
    "    \n",
    "    f.write(f\"\\n\\nMEILLEUR MOD√àLE: {best_model['model']}\\n\")\n",
    "    f.write(f\"  F-mesure: {best_model['f1']:.4f}\\n\")\n",
    "    f.write(f\"  Pr√©cision: {best_model['precision']:.4f}\\n\")\n",
    "    f.write(f\"  Rappel: {best_model['recall']:.4f}\\n\")\n",
    "    f.write(f\"  TP (fraudes d√©tect√©es): {best_model['tp']:,}\\n\")\n",
    "    f.write(f\"  FN (fraudes manqu√©es): {best_model['fn']:,}\\n\")\n",
    "    f.write(f\"  FP (fausses alarmes): {best_model['fp']:,}\\n\")\n",
    "\n",
    "print(\"‚úì R√©sum√© sauvegard√©: resume_modelisation.txt\")\n",
    "\n",
    "print(\"\\n‚úÖ Tous les r√©sultats ont √©t√© sauvegard√©s!\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
